import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

import openai
from backend.src.config.config import OPENAI_API_KEY

openai.api_key = OPENAI_API_KEY

class ChatGPTClient:
    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.0) -> None:
        """
        Initialize ChatGPT client.

        Args:
            model_name (str): The name of the model to be used.
            temperature (float): The temperature parameter for response generation.
        """
        self.model_name = model_name
        self.temperature = temperature

    def create_user_prompt(self, user_prompt_template: str, user_inquiry: str, file_content: str) -> str:
        """
        Formats the user prompt by filling in the placeholders in the template.

        Args:
            user_prompt_template (str): The template string containing placeholders for the inquiry and file content.
            user_inquiry (str): The specific user inquiry or question to be filled into the template.
            file_content (str): The content of the file to be included in the prompt.

        Returns:
            str: The formatted user prompt with the placeholders replaced by the user inquiry and file content.
        """
        try:
            return user_prompt_template.format(user_inquiry=user_inquiry, file_content=file_content)
        except KeyError as e:
            raise ValueError(f"Missing key in template: {e}")

    def handle_chat(self, system_prompt: str, user_prompt: str, max_tokens: int = 5000) -> str:
        """
        Handles the chat by invoking the model with the system and user prompts.

        Args:
            system_prompt (str): The system prompt that provides context or guidance for the LLM.
            user_prompt (str): The user's input or question to be sent to the model.
            max_tokens (int, optional): The maximum number of tokens in the model's response. Defaults to 5000.

        Returns:
            str: The response generated by the LLM based on the prompts.
        """
        try:
            client = openai.OpenAI()

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=max_tokens
            )

            return response.choices[0].message.content.strip()

        except openai.OpenAIError as e:
            raise RuntimeError(f"OpenAI API Error: {e}") 
        
        except Exception as e:
            raise RuntimeError(f"Unexpected error: {e}")